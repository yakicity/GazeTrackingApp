import json
import math
import time
from datetime import datetime
from typing import Dict, Tuple, List, Optional

import av
import cv2
import mediapipe as mp
import numpy as np
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
from fpdf import FPDF
import os
import io
from PIL import Image, ImageDraw, ImageFont

# ================= è¦–ç·šæ¨å®šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =================
class GazeEstimator:
    LEFT_EYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]
    RIGHT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
    LEFT_IRIS = [474, 475, 476, 477, 468, 469, 470, 471, 472]
    RIGHT_IRIS = [469, 470, 471, 472, 473, 474, 475, 476, 477]

    def __init__(self, det_conf=0.5, trk_conf=0.5):
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=det_conf,
            min_tracking_confidence=trk_conf,
        )
        # ---- ã‚­ãƒ£ãƒªãƒ–ä¿å­˜ï¼ˆraw_vecã®åŸºæº–å€¤ & ãƒã‚¤ã‚¢ã‚¹è¨ˆç®—ç”¨ã®å¹³å‡eyeåº§æ¨™ï¼‰ ----
        self.rx_left: Optional[float] = None
        self.rx_right: Optional[float] = None
        self.ry_top: Optional[float] = None
        self.ry_bottom: Optional[float] = None
        # ãƒã‚¤ã‚¢ã‚¹ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰: gx = raw_x * scale_x + offx, gy = raw_y * scale_y + offy
        self.offx_px: float = 0.0
        self.offy_px: float = 0.0
        # ã‚­ãƒ£ãƒªãƒ–æ¸ˆã¿ãƒ•ãƒ©ã‚°
        self.calibrated: bool = False
        # å‚è€ƒ: ã‚­ãƒ£ãƒªãƒ–æ™‚ã®ç”»é¢ã‚µã‚¤ã‚ºï¼ˆã‚ªãƒ•ã‚»ãƒƒãƒˆå†ç¾ç”¨ï¼‰
        self.base_w: Optional[int] = None
        self.base_h: Optional[int] = None

    @staticmethod
    def _center(landmarks, idxs):
        pts = np.array([[landmarks[i].x, landmarks[i].y] for i in idxs if i < len(landmarks)])
        if len(pts) == 0:
            return None
        return pts.mean(axis=0)

    def _iris_center_circle_px(self, landmarks, indices, w, h):
        pts = [(landmarks[i].x * w, landmarks[i].y * h) for i in indices if i < len(landmarks)]
        if len(pts) < 3:
            return None
        pts_np = np.array(pts, dtype=np.float32)
        (cx, cy), _ = cv2.minEnclosingCircle(pts_np)
        return (cx, cy)  # pixels

    def set_calibration_values(
        self,
        rx_left: float, rx_right: float,
        ry_top: float, ry_bottom: float,
        offx_px: float, offy_px: float,
        base_w: int, base_h: int
    ):
        """å·¦å³ãƒ»ä¸Šä¸‹ã®raw_vecåŸºæº–ã¨ã€ãƒ”ã‚¯ã‚»ãƒ«ã‚ªãƒ•ã‚»ãƒƒãƒˆã€åŸºæº–è§£åƒåº¦ã‚’ä¿å­˜"""
        self.rx_left = rx_left
        self.rx_right = rx_right
        self.ry_top = ry_top
        self.ry_bottom = ry_bottom
        self.offx_px = offx_px
        self.offy_px = offy_px
        self.base_w = base_w
        self.base_h = base_h
        self.calibrated = True

    def estimate(self, frame_bgr: np.ndarray):
        """
        æˆ»ã‚Š: ok, eye_px, end_px, raw_vec_meas
        - raw_vec_meas = (rx, ry) = iris_center - eye_center [0..1æ­£è¦åŒ–åº§æ¨™ç³»]
        - ã‚­ãƒ£ãƒªãƒ–æ¸ˆã¿ãªã‚‰:
            scale_x = (w - 10) / (rx_right - rx_left)
            scale_y = (h - 10) / (ry_bottom - ry_top)
            gx = rx * scale_x + offx_px
            gy = ry * scale_y + offy_px
          æœªã‚­ãƒ£ãƒªãƒ–æ™‚ã¯ç°¡æ˜“æ—¢å®šï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ï¼‰ã§å‹•ä½œ
        """
        h, w, _ = frame_bgr.shape
        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
        res = self.face_mesh.process(rgb)
        if not res.multi_face_landmarks:
            return False, None, None, None
        lm = res.multi_face_landmarks[0].landmark

        # ç›®ä¸­å¿ƒ
        lc = self._center(lm, self.LEFT_EYE)
        rc = self._center(lm, self.RIGHT_EYE)
        if lc is None or rc is None:
            return False, None, None, None
        eye_center = (lc + rc) / 2.0
        eye_px = (int(eye_center[0] * w), int(eye_center[1] * h))

        # è™¹å½©ä¸­å¿ƒï¼ˆå††ã‚ã¦ã¯ã‚ï¼‰
        li_px = self._iris_center_circle_px(lm, self.LEFT_IRIS, w, h)
        ri_px = self._iris_center_circle_px(lm, self.RIGHT_IRIS, w, h)
        if li_px is None or ri_px is None:
            return False, None, None, None
        iris_px = ((li_px[0] + ri_px[0]) * 0.5, (li_px[1] + ri_px[1]) * 0.5)

        # ç”Ÿraw_vec
        iris_center = np.array([iris_px[0] / w, iris_px[1] / h])
        eye_center_n = np.array([eye_center[0], eye_center[1]])
        # ã€Œç›®ã®ä¸­å¿ƒã€ã‹ã‚‰ã€Œè™¹å½©ï¼ˆã²ã¨ã¿ï¼‰ã®ä¸­å¿ƒã€ã«å‘ã‹ã†ãƒ™ã‚¯ãƒˆãƒ«
        raw_vec = iris_center - eye_center_n  # (rx, ry)
        # ã‚¹ã‚±ãƒ¼ãƒ«ï¼†ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        if self.calibrated and self.rx_left is not None and self.rx_right is not None \
           and self.ry_top is not None and self.ry_bottom is not None:
            # æ¯ãƒ•ãƒ¬ãƒ¼ãƒ ã®è§£åƒåº¦ã‹ã‚‰å‹•çš„ã«ç®—å‡º
            dx = (self.rx_right - self.rx_left)
            dy = (self.ry_bottom - self.ry_top)
            # å®‰å…¨
            if abs(dx) < 1e-9 or abs(dy) < 1e-9:
                scale_x = (w - 10) / 0.01  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                scale_y = (h - 10) / 0.01
            else:
                scale_x = (w - 10) / dx
                scale_y = (h - 10) / dy
            gx = raw_vec[0] * scale_x + self.offx_px
            gy = raw_vec[1] * scale_y + self.offy_px
        else:
            # æœªã‚­ãƒ£ãƒªãƒ–ã®ç°¡æ˜“æ—¢å®šï¼ˆä»»æ„ã®æš«å®šå€¤ï¼‰
            scale_x = (w - 10) / 0.01
            scale_y = (h - 10) / 0.001
            gx = raw_vec[0] * scale_x
            gy = raw_vec[1] * scale_y

        end_px = (
            int(np.clip(eye_px[0] + gx, 10, w - 10)),
            int(np.clip(eye_px[1] + gy, 10, h - 10)),
        )
        return True, eye_px, end_px, (float(raw_vec[0]), float(raw_vec[1]))


# ================== Region é›†è¨ˆ ==================
class RegionTimer:
    def __init__(self, num_regions: int):
        self.num_regions = num_regions
        # æŒ‡å®šã•ã‚ŒãŸãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ•°ã§statsè¾æ›¸ã‚’åˆæœŸåŒ–
        self.stats = {i: 0.0 for i in range(1, self.num_regions + 1)}
        self.last_ts = None

    def region_of(self, x: int, y: int, w: int, h: int) -> int:
        if self.num_regions == 4:
            cx, cy = w // 2, h // 2
            if x < cx and y < cy: return 1
            if x >= cx and y < cy: return 2
            if x < cx and y >= cy: return 3
            return 4
        elif self.num_regions == 3:
            # ç¸¦ã«3åˆ†å‰²
            if x < w / 3: return 1
            if x < 2 * w / 3: return 2
            return 3
        return 1

    def update(self, gaze_px: Tuple[int, int], w: int, h: int):
        now = time.time()
        if self.last_ts is None:
            self.last_ts = now
            return
        dt = now - self.last_ts
        r = self.region_of(gaze_px[0], gaze_px[1], w, h)
        self.stats[r] += dt
        self.last_ts = now

def y_intercept(rx_left, rx_right, w):
    if abs(rx_right - rx_left) < 1e-9: return 0, 0
    a = w / (rx_right - rx_left)
    b = -(w/2) * (rx_right + rx_left) / (rx_right - rx_left)
    return a, b

# ================== â˜…â˜…â˜… ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆã‚¯ãƒ©ã‚¹ (æ–°è¦è¿½åŠ ) â˜…â˜…â˜… ==================
class HeatmapGenerator:
    def __init__(self, width: int, height: int):
        self.width = width
        self.height = height
        self.heatmap_data = np.zeros((height, width), dtype=np.float64)
        self.decay_rate = 0.98
        self.intensity = 5.0
        self.radius = int(max(width, height) * 0.15) # ãƒŸãƒ‹ãƒãƒƒãƒ—å†…ã§ã®åŠå¾„ã‚’å°‘ã—å¤§ãã

    def update(self, gaze_point: Tuple[int, int]):
        x, y = gaze_point
        if x is None or y is None: return

        # NumPyã®ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã¨meshgridã‚’ä½¿ã£ã¦é«˜é€Ÿã«ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‚’é©ç”¨
        x_grid, y_grid = np.ogrid[:self.height, :self.width]
        dist_sq = (x_grid - y)**2 + (y_grid - x)**2

        gauss = self.intensity * np.exp(-dist_sq / (2 * (self.radius / 2)**2))
        self.heatmap_data += gauss

    def decay(self):
        self.heatmap_data *= self.decay_rate
        self.heatmap_data[self.heatmap_data < 0.01] = 0

    def generate_minimap_image(self, current_gaze_point: Optional[Tuple[int, int]]) -> np.ndarray:
        # 1. ãƒŸãƒ‹ãƒãƒƒãƒ—ã®èƒŒæ™¯ã‚’ä½œæˆ
        minimap_bg = np.zeros((self.height, self.width, 3), dtype=np.uint8)
        cv2.rectangle(minimap_bg, (0, 0), (self.width, self.height), (30, 30, 30), -1)

        # 2. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»
        if np.max(self.heatmap_data) > 0:
            norm_heatmap = self.heatmap_data / np.max(self.heatmap_data)
            heatmap_8u = (norm_heatmap * 255).astype(np.uint8)
            heatmap_color = cv2.applyColorMap(heatmap_8u, cv2.COLORMAP_JET)
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’èƒŒæ™¯ã«é‡ã­åˆã‚ã›
            minimap_bg = cv2.addWeighted(minimap_bg, 0.5, heatmap_color, 0.5, 0)

        # 3. ã‚°ãƒªãƒƒãƒ‰ç·šã‚’æç”»
        for i in range(1, 4):
            x = (i * self.width) // 4
            cv2.line(minimap_bg, (x, 0), (x, self.height), (80, 80, 80), 1)
        for i in range(1, 3):
            y = (i * self.height) // 3
            cv2.line(minimap_bg, (0, y), (self.width, y), (80, 80, 80), 1)

        # 4. ç¾åœ¨ã®è¦–ç·šä½ç½®ã‚’æç”» (èµ¤ã„ç‚¹)
        if current_gaze_point:
            cv2.circle(minimap_bg, current_gaze_point, 5, (0, 0, 255), -1)
            cv2.circle(minimap_bg, current_gaze_point, 7, (255, 255, 255), 1)

        # 5. å¤–æ ã‚’æç”»
        cv2.rectangle(minimap_bg, (0, 0), (self.width - 1, self.height - 1), (255, 255, 255), 1)

        return minimap_bg

# ================ Video Processorï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ»ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å¯¾å¿œï¼‰ =================
class VideoProcessor:
    def __init__(self):
        self.est = GazeEstimator(0.5, 0.5)
        self.running = False

        # ---- ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ ----
        # å››éš…: å·¦ä¸Šâ†’å³ä¸Šâ†’å·¦ä¸‹â†’å³ä¸‹
        self.targets = [
            ("å·¦ä¸Š", 0.10, 0.10),
            ("å³ä¸Š", 0.90, 0.10),
            ("å·¦ä¸‹", 0.10, 0.90),
            ("å³ä¸‹", 0.90, 0.90),
        ]
        self.calibrating = False
        self.target_idx = 0
        self.capture_request = False
        self.capture_buffer: List[Tuple[float, float, int, int]] = []  # (rx, ry, eye_x, eye_y)
        # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜: name -> å¹³å‡ (rx, ry, eye_x, eye_y)
        self.samples: Dict[str, Tuple[float, float, float, float]] = {}
        # ç›´è¿‘ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚º
        self.last_w: Optional[int] = None
        self.last_h: Optional[int] = None
        self.calib_ready = False  # â˜… å…¨ç‚¹å–å¾—æ¸ˆã¿ã‹ã©ã†ã‹
        # FPSè¨ˆæ¸¬ç”¨ã®å¤‰æ•°
        self.fps_start_time = 0
        self.fps_frame_count = 0
        self.fps = 0.0

        # â˜…â˜…â˜… ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—é–¢é€£ã®å¤‰æ•°ã‚’è¿½åŠ  â˜…â˜…â˜…
        self.minimap_width = 220
        self.minimap_height = 140
        self.heatmap_generator = HeatmapGenerator(self.minimap_width, self.minimap_height)
        self.show_heatmap = True # UIã‹ã‚‰å¤‰æ›´ã•ã‚Œã‚‹

        # UIã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ã®ã‚’å¾…ã¤ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
        self.num_regions = st.session_state.get("num_regions", 4)
        self.timer = RegionTimer(self.num_regions)


    def start_calibration(self):
        self.calibrating = True
        self.calib_ready = False
        self.target_idx = 0
        self.capture_request = False
        self.capture_buffer.clear()
        self.samples.clear()

    def request_sample(self):
        if self.calibrating and self.target_idx < len(self.targets):
            self.capture_request = True
            self.capture_buffer.clear()

    def skip_target(self):
        if self.calibrating and self.target_idx < len(self.targets):
            self.target_idx += 1
            self.capture_request = False
            self.capture_buffer.clear()
            if self.target_idx >= len(self.targets):
                self.calib_ready = True  # å…¨ç‚¹çµ‚ã‚ã‚Š

    def reset_calibration(self):
        self.start_calibration()

    def _compute_and_apply_calibration(self) -> Tuple[bool, str]:
        """
        å››éš…ã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰:
          æ°´å¹³ãƒ¬ãƒ³ã‚¸: rx_left (=å·¦ä¸Š/å·¦ä¸‹ã®å¹³å‡) ã¨ rx_right (=å³ä¸Š/å³ä¸‹ã®å¹³å‡)
          å‚ç›´ãƒ¬ãƒ³ã‚¸: ry_top  (=å·¦ä¸Š/å³ä¸Šã®å¹³å‡) ã¨ ry_bottom(=å·¦ä¸‹/å³ä¸‹ã®å¹³å‡)
        ãƒã‚¤ã‚¢ã‚¹:
          offx = å¹³å‡( target_x_left - eye_x_left - rx_left * scale_x )
          offy = å¹³å‡( target_y_top  - eye_y_top  - ry_top  * scale_y )
        """
        need = {"å·¦ä¸Š", "å³ä¸Š", "å·¦ä¸‹", "å³ä¸‹"}
        if not need.issubset(self.samples.keys()):
            return False, "å››éš…ã®ã‚µãƒ³ãƒ—ãƒ«ãŒæƒã£ã¦ã„ã¾ã›ã‚“ã€‚"
        if self.last_w is None or self.last_h is None:
            return False, "ã‚«ãƒ¡ãƒ©ã‹ã‚‰æ˜ åƒãŒå–å¾—ã§ãã¦ã„ã¾ã›ã‚“ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†åº¦è©¦ã—ã¦ãã ã•ã„ã€‚"

        # å¹³å‡ã‚’å–ã‚Šå‡ºã—
        rx_lu, ry_lu, ex_lu, ey_lu = self.samples["å·¦ä¸Š"]
        rx_ru, ry_ru, ex_ru, ey_ru = self.samples["å³ä¸Š"]
        rx_ld, ry_ld, ex_ld, ey_ld = self.samples["å·¦ä¸‹"]
        rx_rd, ry_rd, ex_rd, ey_rd = self.samples["å³ä¸‹"]

        # æ¨ªãƒ¬ãƒ³ã‚¸ï¼ˆå·¦ç¾¤/å³ç¾¤ï¼‰
        rx_left = (rx_lu + rx_ld) / 2.0
        rx_right = (rx_ru + rx_rd) / 2.0
        # ç¸¦ãƒ¬ãƒ³ã‚¸ï¼ˆä¸Šç¾¤/ä¸‹ç¾¤ï¼‰
        ry_top = (ry_lu + ry_ru) / 2.0
        ry_bottom = (ry_ld + ry_rd) / 2.0

        # ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—ã«ä½¿ã†ç¾åœ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚º
        w, h = self.last_w, self.last_h
        dx = rx_right - rx_left
        dy = ry_bottom - ry_top
        if abs(dx) < 1e-9 or abs(dy) < 1e-9:
            return False, "å·®åˆ†ãŒå°ã•ã™ãã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚’å–ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚"
        scale_x = y_intercept(rx_left, rx_right, w)[0]
        scale_y = y_intercept(ry_top, ry_bottom, h)[0]
        offx = y_intercept(rx_left, rx_right, w)[1]
        offy = y_intercept(ry_top, ry_bottom, h)[1]
        # scale_x, offx = y_intercept(rx_left, rx_right, w)
        # scale_y, offy = y_intercept(ry_top, ry_bottom, h)
        # if scale_x is None: return False, "y_intercept calculation failed for x"
        # if scale_y is None:
        #     # Fallback or specific logic for ry_top/ry_bottom if needed
        #     ry_dx = ry_bottom - ry_top
        #     if abs(ry_dx) < 1e-9: return False, "Vertical difference too small"
        #     scale_y = h / ry_dx
        #     offy = - (h/2) * (ry_bottom + ry_top) / ry_dx

        # æ¨å®šå€¤ã‚’ä¿å­˜ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã¯ estimate() å†…ã§æ¯ãƒ•ãƒ¬ãƒ¼ãƒ  w,h ã‹ã‚‰å†è¨ˆç®—ï¼‰
        self.est.set_calibration_values(
            rx_left=rx_left, rx_right=rx_right,
            ry_top=ry_top, ry_bottom=ry_bottom,
            offx_px=float(offx), offy_px=float(offy),
            base_w=w, base_h=h
        )
        return True, (f"é©ç”¨ OK: dx={dx:.6g}, dy={dy:.6g}, "
                      f"offx={offx:.1f}px, offy={offy:.1f}px, "
                      f"scale_x={scale_x:.1f}, scale_y={scale_y:.1f} @ {w}x{h},"
                      f" samples={rx_left, rx_right, ry_top, ry_bottom}")


    def _draw_guides(self, frame):
        h, w, _ = frame.shape
        if self.num_regions == 4:
            cv2.line(frame, (w//2, 0), (w//2, h), (0, 0, 255), 2)
            cv2.line(frame, (0, h//2), (w, h//2), (0, 0, 255), 2)
        elif self.num_regions == 3:
            cv2.line(frame, (w//3, 0), (w//3, h), (0, 0, 255), 2)
            cv2.line(frame, (2*w//3, 0), (2*w//3, h), (0, 0, 255), 2)

    def _draw_calib_target(self, frame):
        if not self.calibrating:
            return
        h, w, _ = frame.shape

        if self.calib_ready or self.target_idx >= len(self.targets):
            return
        name, nx, ny = self.targets[self.target_idx]
        px, py = int(nx * w), int(ny * h)
        cv2.circle(frame, (px, py), 16, (0, 255, 0), 3)
        cv2.circle(frame, (px, py), 6, (0, 255, 255), -1)

    def draw_overlay(self, frame: np.ndarray, eye_px, end_px):
        if eye_px and end_px:
            cv2.circle(frame, eye_px, 6, (255, 255, 255), -1)
            cv2.circle(frame, end_px, 6, (0, 255, 255), -1)
            steps = 8
            for i in range(steps):
                t = i / (steps - 1)
                x = int(eye_px[0] + t * (end_px[0] - eye_px[0]))
                y = int(eye_px[1] + t * (end_px[1] - eye_px[1]))
                cv2.circle(frame, (x, y), max(2, 6 - i), (0, 200, 255), -1)
        return frame

    # ---- WebRTC ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç† ----
    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)
        h, w, _ = img.shape
        self.last_w, self.last_h = img.shape[1], img.shape[0]
        # print("b",img.shape[1], img.shape[0])

        ok, eye_px, end_px, raw_vec = self.est.estimate(img)
        # FPSã‚’è¨ˆç®—
        if self.fps_start_time == 0:
            self.fps_start_time = time.time()
        self.fps_frame_count += 1
        elapsed_time = time.time() - self.fps_start_time
        if elapsed_time >= 1.0: # 1ç§’ã”ã¨ã«æ›´æ–°
            self.fps = self.fps_frame_count / elapsed_time
            self.fps_frame_count = 0
            self.fps_start_time = time.time()

        # ã‚­ãƒ£ãƒªãƒ–åé›†
        if self.calibrating and self.capture_request and ok and raw_vec:
            self.capture_buffer.append((raw_vec[0], raw_vec[1], eye_px[0], eye_px[1]))
            if len(self.capture_buffer) >= 15:
                avg = np.mean(self.capture_buffer, axis=0)
                name, _, _ = self.targets[self.target_idx]
                self.samples[name] = tuple(float(v) for v in avg)
                self.capture_request = False
                self.capture_buffer.clear()
                self.target_idx += 1
                if self.target_idx >= len(self.targets): self.calib_ready = True

        # â˜…â˜…â˜… ãƒŸãƒ‹ãƒãƒƒãƒ—ç”¨ã«è¦–ç·šåº§æ¨™ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¦æ›´æ–° â˜…â˜…â˜…
        current_minimap_gaze = None
        if self.running and ok and end_px:
            self.timer.update(end_px, w, h)
            if self.show_heatmap:
                # ç”»é¢å…¨ä½“ã®åº§æ¨™ã‚’ãƒŸãƒ‹ãƒãƒƒãƒ—åº§æ¨™ã«å¤‰æ›
                minimap_x = int((end_px[0] / w) * self.minimap_width)
                minimap_y = int((end_px[1] / h) * self.minimap_height)
                current_minimap_gaze = (minimap_x, minimap_y)
                self.heatmap_generator.update(current_minimap_gaze)

        # â˜…â˜…â˜… ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æ¸›è¡°ã•ã›ã€ãƒŸãƒ‹ãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã¦è²¼ã‚Šä»˜ã‘ â˜…â˜…â˜…
        if self.show_heatmap:
            self.heatmap_generator.decay()
            minimap_image = self.heatmap_generator.generate_minimap_image(current_minimap_gaze)

            # ç”»é¢å³ä¸‹ã«ãƒŸãƒ‹ãƒãƒƒãƒ—ã‚’é…ç½®
            margin = 10
            map_h, map_w, _ = minimap_image.shape
            y_offset = h - map_h - margin
            x_offset = w - map_w - margin

            # NumPyã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã§ç”»åƒã‚’åˆæˆ
            img[y_offset:y_offset + map_h, x_offset:x_offset + map_w] = minimap_image

        # æç”»å‡¦ç†
        img = self.draw_overlay(img, eye_px if ok else None, end_px if ok else None)
        self._draw_guides(img)

        if self.calibrating and not self.calib_ready and self.target_idx < len(self.targets):
            name, nx, ny = self.targets[self.target_idx]
            px, py = int(nx * w), int(ny * h)
            cv2.circle(img, (px, py), 16, (0, 255, 0), 3)
            cv2.circle(img, (px, py), 6, (0, 255, 255), -1)

        fps_text = f"FPS: {self.fps:.1f}"
        cv2.putText(img, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

        return av.VideoFrame.from_ndarray(img, format="bgr24")

        # self._draw_guides(img)
        # self._draw_calib_target(img)

        # fps_text = f"FPS: {self.fps:.1f}"
        # cv2.putText(img, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        # return av.VideoFrame.from_ndarray(img, format="bgr24")


    # UIå´ã‹ã‚‰å‘¼ã¶
    def apply_calibration(self) -> Tuple[bool, str]:
        return self._compute_and_apply_calibration()

def get_region_labels(num_regions: int) -> Dict[int, str]:
    """ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ•°ã«å¿œã˜ãŸãƒ©ãƒ™ãƒ«ã‚’è¿”ã™"""
    if num_regions == 4:
        return {1: "â‘  å·¦ä¸Š", 2: "â‘¡ å³ä¸Š", 3: "â‘¢ å·¦ä¸‹", 4: "â‘£ å³ä¸‹"}
    if num_regions == 3:
        return {1: "â‘  å·¦", 2: "â‘¡ ä¸­å¤®", 3: "â‘¢ å³"}
    # ä»–ã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ•°ãŒå¿…è¦ãªå ´åˆã¯ã“ã“ã«è¿½åŠ 
    return {i: f"ã‚¨ãƒªã‚¢ {i}" for i in range(1, num_regions + 1)}


# ================= PDF=================
JAPANESE_FONT_PATH = "ipaexg00401/ipaexg.ttf"

def create_history_pdf(history_data: list) -> io.BytesIO:
    pdf = FPDF()
    pdf.add_page()
    pdf.add_font('Japanese', '', JAPANESE_FONT_PATH, uni=True)
    pdf.set_font('Japanese', '', 16)
    pdf.cell(0, 10, "ã‚¢ã‚¤ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°åˆ†æå±¥æ­´", 0, 1, 'C')
    pdf.ln(10)
    for item in reversed(history_data):
        stats = item.get("stats", {})
        num_regions = len(stats)
        if num_regions == 0: continue
        labels = get_region_labels(num_regions)

        pdf.set_font('Japanese', '', 12)
        pdf.cell(0, 10, f"è¨˜éŒ²æ—¥æ™‚: {item['time']}", 0, 1)
        pdf.set_font('Japanese', '', 10)
        pdf.set_fill_color(240, 240, 240)
        cell_width = 60; cell_height = 8
        pdf.cell(cell_width, cell_height, "ç¯„å›²", border=1, fill=True, align='C')
        pdf.cell(cell_width, cell_height, "åˆè¨ˆç§’", border=1, fill=True, align='C')
        pdf.ln()
        for i in range(1, num_regions + 1):
            pdf.cell(cell_width, cell_height, labels.get(i, f"Region {i}"), border=1)
            pdf.cell(cell_width, cell_height, f"{stats.get(i, 0.0):.1f}", border=1)
            pdf.ln()
        pdf.ln(10)
    return io.BytesIO(pdf.output())


# ================= Streamlit UI ==================
st.set_page_config(page_title="ç°¡æ˜“ã‚¢ã‚¤ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç‰ˆï¼‰", layout="wide")
st.title("ç°¡æ˜“ã‚¢ã‚¤ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if "history" not in st.session_state:
    st.session_state.history = []
if "num_regions" not in st.session_state:
    st.session_state.num_regions = 4 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
if "running" not in st.session_state:
    st.session_state.running = False

def on_region_change():
    st.session_state.num_regions = st.session_state.selectbox_regions
    st.session_state.current_stats = {i: 0.0 for i in range(1, st.session_state.num_regions + 1)}


num_regions = st.sidebar.selectbox(
    "åˆ†æã‚¨ãƒªã‚¢ã®åˆ†å‰²æ•°ã‚’é¸æŠ", [3, 4],
    index=[3, 4].index(st.session_state.num_regions),
    on_change=on_region_change,
    key='selectbox_regions'
)

if "current_stats" not in st.session_state or len(st.session_state.current_stats) != st.session_state.num_regions:
    st.session_state.current_stats = {i: 0.0 for i in range(1, st.session_state.num_regions + 1)}

region_labels = get_region_labels(st.session_state.num_regions)



cols = st.columns([1, 1])

with cols[0]:
    st.subheader("ã‚«ãƒ¡ãƒ©æ˜ åƒ")
    # â˜…â˜…â˜… UIã«ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è¿½åŠ  â˜…â˜…â˜…
    show_heatmap_toggle = st.checkbox("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’è¡¨ç¤º", value=True)

    rtc_config = RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})
    ctx = webrtc_streamer(
        key="eyetracking",
        mode=WebRtcMode.SENDRECV,
        video_processor_factory=VideoProcessor,
        async_processing=True,
        rtc_configuration=rtc_config,
        media_stream_constraints={"video": True, "audio": False},
    )

    # â˜…â˜…â˜… ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®çŠ¶æ…‹ã‚’VideoProcessorã«æ¸¡ã™ â˜…â˜…â˜…
    if ctx.state.playing and ctx.video_processor:
        ctx.video_processor.show_heatmap = show_heatmap_toggle
        # VideoProcessorã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ•°ã‚’UIã®é¸æŠã«åŒæœŸ
        if ctx.video_processor.num_regions != st.session_state.num_regions:
            ctx.video_processor.num_regions = st.session_state.num_regions
            ctx.video_processor.timer = RegionTimer(st.session_state.num_regions)


    c1, c2, c3 = st.columns(3)
    with c1:
        if st.button("åˆ†æé–‹å§‹", use_container_width=True):
            if ctx.state.playing:
                st.session_state.running = True
                if ctx.video_processor:
                    # ã‚¿ã‚¤ãƒãƒ¼ã‚’ç¾åœ¨ã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ•°ã§ãƒªã‚»ãƒƒãƒˆ
                    ctx.video_processor.num_regions = st.session_state.num_regions
                    ctx.video_processor.running = True
                    ctx.video_processor.timer = RegionTimer(st.session_state.num_regions)
                    st.session_state.current_stats = ctx.video_processor.timer.stats.copy()
            else: st.warning("ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•ã—ã¦ä¸‹ã•ã„")
    with c2:
        if st.button("åˆ†æçµ‚äº†", use_container_width=True):
            if ctx.state.playing and st.session_state.running:
                st.session_state.running = False
                if ctx.video_processor:
                    ctx.video_processor.running = False
                    stats = ctx.video_processor.timer.stats
                    st.session_state.current_stats = stats
                    st.session_state.history.append({
                        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "stats": stats.copy(),
                    })
            else: st.warning("åˆ†æãŒé–‹å§‹ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    with c3:
        if st.button("ã‚­ãƒ£ãƒªãƒ–é–‹å§‹", use_container_width=True):
            if ctx.state.playing:
                ctx.video_processor.start_calibration()
                st.success("ã‚­ãƒ£ãƒªãƒ–ã‚’é–‹å§‹ã€‚å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¦‹ã¦ã€ã‚µãƒ³ãƒ—ãƒ«å–å¾—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            else: st.warning("ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•ã—ã¦ä¸‹ã•ã„")

    # é€²æ—ã®è¡Œã‚’é †åºé€šã‚Šã«
    if ctx.state.playing and ctx.video_processor and ctx.video_processor.calibrating:
        d1, d2, d3 = st.columns(3)
        with d1:
            if st.button("ã‚µãƒ³ãƒ—ãƒ«å–å¾—ï¼ˆ15fï¼‰", use_container_width=True):
                if not ctx or not ctx.state.playing:
                    st.warning("ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•ã—ã¦ä¸‹ã•ã„")
                elif not ctx.video_processor or not ctx.video_processor.calibrating:
                    st.warning("ã¾ãšã€ã‚­ãƒ£ãƒªãƒ–é–‹å§‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„")
                else:
                    ctx.video_processor.request_sample()
                    st.info("å–å¾—ä¸­â€¦ï¼ˆ15ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰")
        with d2:
            if st.button("ãƒªã‚»ãƒƒãƒˆ"):
                ctx.video_processor.reset_calibration()
        with d3:
            if st.button("é©ç”¨"):
                ok, msg = ctx.video_processor.apply_calibration()
                if ok: st.success(msg)
                else: st.error(msg)

        order = [t[0] for t in ctx.video_processor.targets]
        marks = ["âœ…" if name in ctx.video_processor.samples else "â¬œ" for name in order]
        st.caption("å–å¾—çŠ¶æ³: " + " | ".join(f"{m} {n}" for m, n in zip(marks, order)))
        if ctx.video_processor.calib_ready:
            st.success("å››éš…ã®ã‚µãƒ³ãƒ—ãƒ«ãŒæƒã„ã¾ã—ãŸã€‚ã€é©ç”¨ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        elif ctx.video_processor.target_idx < len(order):
            st.caption(f"æ¬¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {order[ctx.video_processor.target_idx]}")

    if ctx.state.playing and ctx.video_processor and st.session_state.running:
        st.session_state.current_stats = ctx.video_processor.timer.stats
with cols[1]:
    st.subheader("ä»Šå›ã®åˆ†æçµæœ")
    cur = st.session_state.current_stats
    # â˜… ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã‚’å‹•çš„ã«ç”Ÿæˆ
    table_rows = [(region_labels.get(i, f"ã‚¨ãƒªã‚¢ {i}"), cur.get(i, 0.0)) for i in range(1, num_regions + 1)]
    st.table({"ç¯„å›²": [r[0] for r in table_rows], "åˆè¨ˆç§’": [f"{r[1]:.1f}" for r in table_rows]})

    # â˜… JSONå‡ºåŠ›ã‚’å‹•çš„ã«ç”Ÿæˆ
    json_str = json.dumps({
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "results": {
            region_labels.get(i, f"Region {i}"): {"duration_sec": f"{cur.get(i, 0.0):.1f}"}
            for i in range(1, num_regions + 1)
        }
    }, ensure_ascii=False, indent=2)
    st.download_button(
        label="çµæœã‚’JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=json_str,
        file_name=f"gaze_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json",
    )

st.markdown("---")
st.subheader("å±¥æ­´")

if not st.session_state.history:
    st.info("ã¾ã å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã€åˆ†æé–‹å§‹ã€â†’ã€åˆ†æçµ‚äº†ã€ã§è¿½åŠ ã•ã‚Œã¾ã™ã€‚")
else:
    if not os.path.exists(JAPANESE_FONT_PATH):
        st.error(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {JAPANESE_FONT_PATH}\n"
                 "PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
    else:
        pdf_data = create_history_pdf(st.session_state.history)
        st.download_button(
            label="ğŸ“ˆ å…¨ã¦ã®å±¥æ­´ã‚’PDFã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=pdf_data,
            file_name=f"gaze_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
            mime="application/pdf",
        )

    for item in reversed(st.session_state.history):
        with st.expander(f"{item['time']} ã®çµæœ"):
            stats = item.get("stats", {})
            num_regions_in_history = len(stats)
            if num_regions_in_history == 0: continue
            history_labels = get_region_labels(num_regions_in_history)
            rows = [(history_labels.get(i, f"ã‚¨ãƒªã‚¢ {i}"), stats.get(i, 0.0)) for i in history_labels]
            st.table({"ç¯„å›²": [r[0] for r in rows], "åˆè¨ˆç§’": [f"{r[1]:.1f}" for r in rows]})
