import json
import math
import time
from datetime import datetime
from typing import Dict, Tuple, List, Optional

import av
import cv2
import mediapipe as mp
import numpy as np
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
from fpdf import FPDF
import os
import io
from PIL import Image, ImageDraw, ImageFont

# ================= è¦–ç·šæ¨å®šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =================
class GazeEstimator:
    LEFT_EYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]
    RIGHT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]
    LEFT_IRIS = [474, 475, 476, 477, 468, 469, 470, 471, 472]
    RIGHT_IRIS = [469, 470, 471, 472, 473, 474, 475, 476, 477]

    def __init__(self, det_conf=0.5, trk_conf=0.5):
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=det_conf,
            min_tracking_confidence=trk_conf,
        )
        # ---- ã‚­ãƒ£ãƒªãƒ–ä¿å­˜ï¼ˆraw_vecã®åŸºæº–å€¤ & ãƒã‚¤ã‚¢ã‚¹è¨ˆç®—ç”¨ã®å¹³å‡eyeåº§æ¨™ï¼‰ ----
        self.rx_left: Optional[float] = None
        self.rx_right: Optional[float] = None
        self.ry_top: Optional[float] = None
        self.ry_bottom: Optional[float] = None
        # ãƒã‚¤ã‚¢ã‚¹ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰: gx = raw_x * scale_x + offx, gy = raw_y * scale_y + offy
        self.offx_px: float = 0.0
        self.offy_px: float = 0.0
        # ã‚­ãƒ£ãƒªãƒ–æ¸ˆã¿ãƒ•ãƒ©ã‚°
        self.calibrated: bool = False
        # å‚è€ƒ: ã‚­ãƒ£ãƒªãƒ–æ™‚ã®ç”»é¢ã‚µã‚¤ã‚ºï¼ˆã‚ªãƒ•ã‚»ãƒƒãƒˆå†ç¾ç”¨ï¼‰
        self.base_w: Optional[int] = None
        self.base_h: Optional[int] = None

    @staticmethod
    def _center(landmarks, idxs):
        pts = np.array([[landmarks[i].x, landmarks[i].y] for i in idxs if i < len(landmarks)])
        if len(pts) == 0:
            return None
        return pts.mean(axis=0)

    def _iris_center_circle_px(self, landmarks, indices, w, h):
        pts = [(landmarks[i].x * w, landmarks[i].y * h) for i in indices if i < len(landmarks)]
        if len(pts) < 3:
            return None
        pts_np = np.array(pts, dtype=np.float32)
        (cx, cy), _ = cv2.minEnclosingCircle(pts_np)
        return (cx, cy)  # pixels

    def set_calibration_values(
        self,
        rx_left: float, rx_right: float,
        ry_top: float, ry_bottom: float,
        offx_px: float, offy_px: float,
        base_w: int, base_h: int
    ):
        """å·¦å³ãƒ»ä¸Šä¸‹ã®raw_vecåŸºæº–ã¨ã€ãƒ”ã‚¯ã‚»ãƒ«ã‚ªãƒ•ã‚»ãƒƒãƒˆã€åŸºæº–è§£åƒåº¦ã‚’ä¿å­˜"""
        self.rx_left = rx_left
        self.rx_right = rx_right
        self.ry_top = ry_top
        self.ry_bottom = ry_bottom
        self.offx_px = offx_px
        self.offy_px = offy_px
        self.base_w = base_w
        self.base_h = base_h
        self.calibrated = True

    def estimate(self, frame_bgr: np.ndarray):
        """
        æˆ»ã‚Š: ok, eye_px, end_px, raw_vec_meas
        - raw_vec_meas = (rx, ry) = iris_center - eye_center [0..1æ­£è¦åŒ–åº§æ¨™ç³»]
        - ã‚­ãƒ£ãƒªãƒ–æ¸ˆã¿ãªã‚‰:
            scale_x = (w - 10) / (rx_right - rx_left)
            scale_y = (h - 10) / (ry_bottom - ry_top)
            gx = rx * scale_x + offx_px
            gy = ry * scale_y + offy_px
          æœªã‚­ãƒ£ãƒªãƒ–æ™‚ã¯ç°¡æ˜“æ—¢å®šï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ï¼‰ã§å‹•ä½œ
        """
        h, w, _ = frame_bgr.shape
        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
        res = self.face_mesh.process(rgb)
        if not res.multi_face_landmarks:
            return False, None, None, None
        lm = res.multi_face_landmarks[0].landmark

        # ç›®ä¸­å¿ƒ
        lc = self._center(lm, self.LEFT_EYE)
        rc = self._center(lm, self.RIGHT_EYE)
        if lc is None or rc is None:
            return False, None, None, None
        eye_center = (lc + rc) / 2.0
        eye_px = (int(eye_center[0] * w), int(eye_center[1] * h))

        # è™¹å½©ä¸­å¿ƒï¼ˆå††ã‚ã¦ã¯ã‚ï¼‰
        li_px = self._iris_center_circle_px(lm, self.LEFT_IRIS, w, h)
        ri_px = self._iris_center_circle_px(lm, self.RIGHT_IRIS, w, h)
        if li_px is None or ri_px is None:
            return False, None, None, None
        iris_px = ((li_px[0] + ri_px[0]) * 0.5, (li_px[1] + ri_px[1]) * 0.5)

        # ç”Ÿraw_vec
        iris_center = np.array([iris_px[0] / w, iris_px[1] / h])
        eye_center_n = np.array([eye_center[0], eye_center[1]])
        # ã€Œç›®ã®ä¸­å¿ƒã€ã‹ã‚‰ã€Œè™¹å½©ï¼ˆã²ã¨ã¿ï¼‰ã®ä¸­å¿ƒã€ã«å‘ã‹ã†ãƒ™ã‚¯ãƒˆãƒ«
        raw_vec = iris_center - eye_center_n  # (rx, ry)
        # ã‚¹ã‚±ãƒ¼ãƒ«ï¼†ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        if self.calibrated and self.rx_left is not None and self.rx_right is not None \
           and self.ry_top is not None and self.ry_bottom is not None:
            # æ¯ãƒ•ãƒ¬ãƒ¼ãƒ ã®è§£åƒåº¦ã‹ã‚‰å‹•çš„ã«ç®—å‡º
            dx = (self.rx_right - self.rx_left)
            dy = (self.ry_bottom - self.ry_top)
            # å®‰å…¨
            if abs(dx) < 1e-9 or abs(dy) < 1e-9:
                scale_x = (w - 10) / 0.01  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                scale_y = (h - 10) / 0.01
            else:
                scale_x = (w - 10) / dx
                scale_y = (h - 10) / dy
            gx = raw_vec[0] * scale_x + self.offx_px
            gy = raw_vec[1] * scale_y + self.offy_px
        else:
            # æœªã‚­ãƒ£ãƒªãƒ–ã®ç°¡æ˜“æ—¢å®šï¼ˆä»»æ„ã®æš«å®šå€¤ï¼‰
            scale_x = (w - 10) / 0.01
            scale_y = (h - 10) / 0.001
            gx = raw_vec[0] * scale_x
            gy = raw_vec[1] * scale_y

        end_px = (
            int(np.clip(eye_px[0] + gx, 10, w - 10)),
            int(np.clip(eye_px[1] + gy, 10, h - 10)),
        )
        return True, eye_px, end_px, (float(raw_vec[0]), float(raw_vec[1]))


# ================== Region é›†è¨ˆ ==================
class RegionTimer:
    def __init__(self):
        self.stats = {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}
        self.last_ts = None

    @staticmethod
    def region_of(x: int, y: int, w: int, h: int) -> int:
        cx, cy = w // 2, h // 2
        if x < cx and y < cy:
            return 1
        if x >= cx and y < cy:
            return 2
        if x < cx and y >= cy:
            return 3
        return 4

    def update(self, gaze_px: Tuple[int, int], w: int, h: int):
        now = time.time()
        if self.last_ts is None:
            self.last_ts = now
            return
        dt = now - self.last_ts
        r = self.region_of(gaze_px[0], gaze_px[1], w, h)
        self.stats[r] += dt
        self.last_ts = now
def y_intercept(rx_left, rx_right, w):
    a = w / (rx_right - rx_left)  # å‚¾ã
    b = -(w/2) * (rx_right + rx_left) / (rx_right - rx_left)  # yåˆ‡ç‰‡
    return a, b

# ================ Video Processorï¼ˆã‚­ãƒ£ãƒªãƒ–å¯¾å¿œï¼‰ =================
class VideoProcessor:
    def __init__(self):
        self.est = GazeEstimator(0.5, 0.5)
        self.timer = RegionTimer()
        self.running = False

        # ---- ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ ----
        # å››éš…: å·¦ä¸Šâ†’å³ä¸Šâ†’å·¦ä¸‹â†’å³ä¸‹
        self.targets = [
            ("å·¦ä¸Š", 0.10, 0.10),
            ("å³ä¸Š", 0.90, 0.10),
            ("å·¦ä¸‹", 0.10, 0.90),
            ("å³ä¸‹", 0.90, 0.90),
        ]
        self.calibrating = False
        self.target_idx = 0
        self.capture_request = False
        self.capture_buffer: List[Tuple[float, float, int, int]] = []  # (rx, ry, eye_x, eye_y)
        # ã‚µãƒ³ãƒ—ãƒ«ä¿å­˜: name -> å¹³å‡ (rx, ry, eye_x, eye_y)
        self.samples: Dict[str, Tuple[float, float, float, float]] = {}
        # ç›´è¿‘ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚º
        self.last_w: Optional[int] = None
        self.last_h: Optional[int] = None
        self.calib_ready = False  # â˜… å…¨ç‚¹å–å¾—æ¸ˆã¿ã‹ã©ã†ã‹
        # FPSè¨ˆæ¸¬ç”¨ã®å¤‰æ•°
        self.fps_start_time = 0
        self.fps_frame_count = 0
        self.fps = 0.0

    def start_calibration(self):
        self.calibrating = True
        self.calib_ready = False
        self.target_idx = 0
        self.capture_request = False
        self.capture_buffer.clear()
        self.samples.clear()

    def request_sample(self):
        if self.calibrating and self.target_idx < len(self.targets):
            self.capture_request = True
            self.capture_buffer.clear()

    def skip_target(self):
        if self.calibrating and self.target_idx < len(self.targets):
            self.target_idx += 1
            self.capture_request = False
            self.capture_buffer.clear()
            if self.target_idx >= len(self.targets):
                self.calib_ready = True  # å…¨ç‚¹çµ‚ã‚ã‚Š

    def reset_calibration(self):
        self.start_calibration()

    def _compute_and_apply_calibration(self) -> Tuple[bool, str]:
        """
        å››éš…ã®ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰:
          æ°´å¹³ãƒ¬ãƒ³ã‚¸: rx_left (=å·¦ä¸Š/å·¦ä¸‹ã®å¹³å‡) ã¨ rx_right (=å³ä¸Š/å³ä¸‹ã®å¹³å‡)
          å‚ç›´ãƒ¬ãƒ³ã‚¸: ry_top  (=å·¦ä¸Š/å³ä¸Šã®å¹³å‡) ã¨ ry_bottom(=å·¦ä¸‹/å³ä¸‹ã®å¹³å‡)
        ãƒã‚¤ã‚¢ã‚¹:
          offx = å¹³å‡( target_x_left - eye_x_left - rx_left * scale_x )
          offy = å¹³å‡( target_y_top  - eye_y_top  - ry_top  * scale_y )
        """
        need = {"å·¦ä¸Š", "å³ä¸Š", "å·¦ä¸‹", "å³ä¸‹"}
        if not need.issubset(self.samples.keys()):
            return False, "å››éš…ã®ã‚µãƒ³ãƒ—ãƒ«ãŒæƒã£ã¦ã„ã¾ã›ã‚“ã€‚"
        if self.last_w is None or self.last_h is None:
            return False, "ã‚«ãƒ¡ãƒ©ã‹ã‚‰æ˜ åƒãŒå–å¾—ã§ãã¦ã„ã¾ã›ã‚“ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†åº¦è©¦ã—ã¦ãã ã•ã„ã€‚"

        # å¹³å‡ã‚’å–ã‚Šå‡ºã—
        rx_lu, ry_lu, ex_lu, ey_lu = self.samples["å·¦ä¸Š"]
        rx_ru, ry_ru, ex_ru, ey_ru = self.samples["å³ä¸Š"]
        rx_ld, ry_ld, ex_ld, ey_ld = self.samples["å·¦ä¸‹"]
        rx_rd, ry_rd, ex_rd, ey_rd = self.samples["å³ä¸‹"]

        # æ¨ªãƒ¬ãƒ³ã‚¸ï¼ˆå·¦ç¾¤/å³ç¾¤ï¼‰
        rx_left = (rx_lu + rx_ld) / 2.0
        rx_right = (rx_ru + rx_rd) / 2.0
        # ç¸¦ãƒ¬ãƒ³ã‚¸ï¼ˆä¸Šç¾¤/ä¸‹ç¾¤ï¼‰
        ry_top = (ry_lu + ry_ru) / 2.0
        ry_bottom = (ry_ld + ry_rd) / 2.0

        # ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—ã«ä½¿ã†ç¾åœ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚º
        w, h = self.last_w, self.last_h
        dx = rx_right - rx_left
        dy = ry_bottom - ry_top
        if abs(dx) < 1e-9 or abs(dy) < 1e-9:
            return False, "å·®åˆ†ãŒå°ã•ã™ãã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚’å–ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚"
        scale_x = y_intercept(rx_left, rx_right, w)[0]
        scale_y = y_intercept(ry_top, ry_bottom, h)[0]
        offx = y_intercept(rx_left, rx_right, w)[1]
        offy = y_intercept(ry_top, ry_bottom, h)[1]

        # æ¨å®šå€¤ã‚’ä¿å­˜ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã¯ estimate() å†…ã§æ¯ãƒ•ãƒ¬ãƒ¼ãƒ  w,h ã‹ã‚‰å†è¨ˆç®—ï¼‰
        self.est.set_calibration_values(
            rx_left=rx_left, rx_right=rx_right,
            ry_top=ry_top, ry_bottom=ry_bottom,
            offx_px=float(offx), offy_px=float(offy),
            base_w=w, base_h=h
        )
        return True, (f"é©ç”¨ OK: dx={dx:.6g}, dy={dy:.6g}, "
                      f"offx={offx:.1f}px, offy={offy:.1f}px, "
                      f"scale_x={scale_x:.1f}, scale_y={scale_y:.1f} @ {w}x{h},"
                      f" samples={rx_left, rx_right, ry_top, ry_bottom}")

    # ---- æç”» ----
    def _draw_guides(self, frame):
        h, w, _ = frame.shape
        cv2.line(frame, (w//2, 0), (w//2, h), (0, 0, 255), 2)
        cv2.line(frame, (0, h//2), (w, h//2), (0, 0, 255), 2)

    def _draw_calib_target(self, frame):
        if not self.calibrating:
            return
        h, w, _ = frame.shape

        if self.calib_ready or self.target_idx >= len(self.targets):
            return
        name, nx, ny = self.targets[self.target_idx]
        px, py = int(nx * w), int(ny * h)
        cv2.circle(frame, (px, py), 16, (0, 255, 0), 3)
        cv2.circle(frame, (px, py), 6, (0, 255, 255), -1)

    def draw_overlay(self, frame: np.ndarray, eye_px, end_px):
        if eye_px and end_px:
            cv2.circle(frame, eye_px, 6, (255, 255, 255), -1)
            cv2.circle(frame, end_px, 6, (0, 255, 255), -1)
            steps = 8
            for i in range(steps):
                t = i / (steps - 1)
                x = int(eye_px[0] + t * (end_px[0] - eye_px[0]))
                y = int(eye_px[1] + t * (end_px[1] - eye_px[1]))
                cv2.circle(frame, (x, y), max(2, 6 - i), (0, 200, 255), -1)
        return frame

    # ---- WebRTC ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç† ----
    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img = cv2.flip(img, 1)
        self.last_w, self.last_h = img.shape[1], img.shape[0]
        # print("b",img.shape[1], img.shape[0])

        ok, eye_px, end_px, raw_vec = self.est.estimate(img)
        # FPSã‚’è¨ˆç®—
        if self.fps_start_time == 0:
            self.fps_start_time = time.time()
        self.fps_frame_count += 1
        elapsed_time = time.time() - self.fps_start_time
        if elapsed_time >= 1.0: # 1ç§’ã”ã¨ã«æ›´æ–°
            self.fps = self.fps_frame_count / elapsed_time
            self.fps_frame_count = 0
            self.fps_start_time = time.time()

        ok, eye_px, end_px, raw_vec = self.est.estimate(img)


        # ã‚­ãƒ£ãƒªãƒ–åé›†
        if self.calibrating and self.capture_request and ok and raw_vec is not None:
            rx, ry = raw_vec
            self.capture_buffer.append((rx, ry, eye_px[0], eye_px[1]))
            if len(self.capture_buffer) >= 15:
                avg = np.mean(self.capture_buffer, axis=0)  # (rx, ry, ex, ey)
                name, _, _ = self.targets[self.target_idx]
                # â˜…ä¸Šæ›¸ãOKï¼ˆå–ã‚Šç›´ã—å¯¾å¿œï¼‰
                self.samples[name] = (float(avg[0]), float(avg[1]), float(avg[2]), float(avg[3]))
                # â˜…æ¬¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¸
                self.capture_request = False
                self.capture_buffer.clear()
                self.target_idx += 1
                if self.target_idx >= len(self.targets):
                    self.calib_ready = True

        if self.running and ok and end_px is not None:
            self.timer.update(end_px, img.shape[1], img.shape[0])

        img = self.draw_overlay(img, eye_px if ok else None, end_px if ok else None)
        self._draw_guides(img)
        self._draw_calib_target(img)

        fps_text = f"FPS: {self.fps:.1f}"
        cv2.putText(img, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

        return av.VideoFrame.from_ndarray(img, format="bgr24")


    # UIå´ã‹ã‚‰å‘¼ã¶
    def apply_calibration(self) -> Tuple[bool, str]:
        return self._compute_and_apply_calibration()


# ================= PDFï¼ˆãã®ã¾ã¾ï¼‰ =================
JAPANESE_FONT_PATH = "ipaexg00401/ipaexg.ttf"

def create_history_pdf(history_data: list) -> io.BytesIO:
    pdf = FPDF()
    pdf.add_page()
    pdf.add_font('Japanese', '', JAPANESE_FONT_PATH, uni=True)
    pdf.set_font('Japanese', '', 16)
    pdf.cell(0, 10, "ã‚¢ã‚¤ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°åˆ†æå±¥æ­´", 0, 1, 'C')
    pdf.ln(10)
    for item in reversed(history_data):
        pdf.set_font('Japanese', '', 12)
        pdf.cell(0, 10, f"è¨˜éŒ²æ—¥æ™‚: {item['time']}", 0, 1)
        pdf.set_font('Japanese', '', 10)
        pdf.set_fill_color(240, 240, 240)
        cell_width = 60; cell_height = 8
        pdf.cell(cell_width, cell_height, "ç¯„å›²", border=1, fill=True, align='C')
        pdf.cell(cell_width, cell_height, "åˆè¨ˆç§’", border=1, fill=True, align='C')
        pdf.ln()
        s = item["stats"]
        rows = [("â‘  å·¦ä¸Š", s[1]), ("â‘¡ å³ä¸Š", s[2]), ("â‘¢ å·¦ä¸‹", s[3]), ("â‘£ å³ä¸‹", s[4])]
        for row in rows:
            pdf.cell(cell_width, cell_height, row[0], border=1)
            pdf.cell(cell_width, cell_height, f"{row[1]:.1f} ç§’", border=1)
            pdf.ln()
        pdf.ln(10)
    return io.BytesIO(pdf.output())


# ================= Streamlit UI ==================
st.set_page_config(page_title="ç°¡æ˜“ã‚¢ã‚¤ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼ˆã‚­ãƒ£ãƒªãƒ–ç‰ˆï¼‰", layout="wide")
st.title("ç°¡æ˜“ã‚¢ã‚¤ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if "history" not in st.session_state:
    st.session_state.history = []
if "current_stats" not in st.session_state:
    st.session_state.current_stats = {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}
if "running" not in st.session_state:
    st.session_state.running = False

cols = st.columns([1, 1])

with cols[0]:
    st.subheader("ã‚«ãƒ¡ãƒ©æ˜ åƒ")
    rtc_config = RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})
    ctx = webrtc_streamer(
        key="eyetracking",
        mode=WebRtcMode.SENDRECV,
        video_processor_factory=VideoProcessor,
        async_processing=True,
        rtc_configuration=rtc_config,
        media_stream_constraints={"video": True, "audio": False},
    )

    c1, c2, c3 = st.columns(3)
    with c1:
        if st.button("åˆ†æé–‹å§‹", use_container_width=True):
            if not ctx or not ctx.state.playing:
                st.warning("ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•ã—ã¦ä¸‹ã•ã„")
            else:
                st.session_state.running = True
                if ctx.video_processor:
                    ctx.video_processor.running = True
                    ctx.video_processor.timer = RegionTimer()
    with c2:
        if st.button("åˆ†æçµ‚äº†", use_container_width=True):
            if not ctx or not ctx.state.playing:
                st.warning("ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•ã—ã¦ä¸‹ã•ã„")
            elif not st.session_state.running:
                st.warning("ã¾ã åˆ†æã¯é–‹å§‹ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            else:
                st.session_state.running = False
                if ctx.video_processor:
                    ctx.video_processor.running = False
                    stats = ctx.video_processor.timer.stats
                    st.session_state.current_stats = stats
                    st.session_state.history.append({
                        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "stats": stats.copy(),
                    })
    with c3:
        if st.button("ã‚­ãƒ£ãƒªãƒ–é–‹å§‹", use_container_width=True):
            if not ctx or not ctx.state.playing:
                st.warning("ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•ã—ã¦ä¸‹ã•ã„")
            else:
                ctx.video_processor.start_calibration()
                st.success("ã‚­ãƒ£ãƒªãƒ–ã‚’é–‹å§‹ã€‚å·¦ä¸Šâ†’å³ä¸Šâ†’å·¦ä¸‹â†’å³ä¸‹ã§ã€ã‚µãƒ³ãƒ—ãƒ«å–å¾—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

    # ã‚­ãƒ£ãƒªãƒ–æ“ä½œ
    if ctx and ctx.video_processor and ctx.video_processor.calibrating:
        d1, d2, d3 = st.columns(3)
        with d1:
            if st.button("ã‚µãƒ³ãƒ—ãƒ«å–å¾—ï¼ˆ15fï¼‰", use_container_width=True):
                if not ctx or not ctx.state.playing:
                    st.warning("ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•ã—ã¦ä¸‹ã•ã„")
                elif not ctx.video_processor or not ctx.video_processor.calibrating:
                    st.warning("ã¾ãšã€ã‚­ãƒ£ãƒªãƒ–é–‹å§‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„")
                else:
                    ctx.video_processor.request_sample()
                    st.info("å–å¾—ä¸­â€¦ï¼ˆ15ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰")
        with d2:
            if st.button("ãƒªã‚»ãƒƒãƒˆ"):
                ctx.video_processor.reset_calibration()
        with d3:
            if st.button("é©ç”¨"):
                ok, msg = ctx.video_processor.apply_calibration()
                if ok:
                    st.success(msg)
                else:
                    st.error(msg)
            if ctx and ctx.video_processor and ctx.video_processor.calibrating:
                # é€²æ—ã®è¡Œã‚’é †åºé€šã‚Šã«
                order = [t[0] for t in ctx.video_processor.targets]
                marks = ["âœ…" if name in ctx.video_processor.samples else "â¬œ" for name in order]
                st.caption("å–å¾—çŠ¶æ³: " + " | ".join([f"{m} {n}" for m, n in zip(marks, order)]))

                # æ¬¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå
                if ctx.video_processor.calib_ready:
                    st.success("å››éš…ã®ã‚µãƒ³ãƒ—ãƒ«ãŒæƒã„ã¾ã—ãŸã€‚ã€é©ç”¨ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
                else:
                    next_name = order[ctx.video_processor.target_idx] if ctx.video_processor.target_idx < len(order) else "â€”"
                    st.caption(f"æ¬¡ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {next_name}")


    # ãƒ©ãƒ³ä¸­ã¯æœ€æ–°å€¤åæ˜ 
    if ctx and ctx.video_processor and st.session_state.running:
        st.session_state.current_stats = ctx.video_processor.timer.stats

with cols[1]:
    st.subheader("ä»Šå›ã®åˆ†æçµæœï¼ˆã©ã“ã‚’ä½•ç§’è¦‹ãŸã‹ï¼‰")
    cur = st.session_state.current_stats
    table_rows = [("â‘  å·¦ä¸Š", cur[1]), ("â‘¡ å³ä¸Š", cur[2]), ("â‘¢ å·¦ä¸‹", cur[3]), ("â‘£ å³ä¸‹", cur[4])]
    st.table({"ç¯„å›²": [r[0] for r in table_rows], "åˆè¨ˆç§’": [f"{r[1]:.1f}" for r in table_rows]})

    json_obj = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "çµæœ": {
            "å¯¾è±¡ç¯„å›²ï¼šâ‘ ": {"åˆè¨ˆç§’æ•°": f"{cur[1]:.1f}s"},
            "å¯¾è±¡ç¯„å›²ï¼šâ‘¡": {"åˆè¨ˆç§’æ•°": f"{cur[2]:.1f}s"},
            "å¯¾è±¡ç¯„å›²ï¼šâ‘¢": {"åˆè¨ˆç§’æ•°": f"{cur[3]:.1f}s"},
            "å¯¾è±¡ç¯„å›²ï¼šâ‘£": {"åˆè¨ˆç§’æ•°": f"{cur[4]:.1f}s"},
        },
    }
    st.download_button(
        label="ä»Šå›ã®çµæœã‚’JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=json.dumps(json_obj, ensure_ascii=False, indent=2),
        file_name=f"gaze_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json",
    )

st.markdown("---")
st.subheader("å±¥æ­´")

if not st.session_state.history:
    st.info("ã¾ã å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã€åˆ†æé–‹å§‹ã€â†’ã€åˆ†æçµ‚äº†ã€ã§è¿½åŠ ã•ã‚Œã¾ã™ã€‚")
else:
    if not os.path.exists(JAPANESE_FONT_PATH):
        st.error(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {JAPANESE_FONT_PATH}\n"
                 "PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
    else:
        pdf_data = create_history_pdf(st.session_state.history)
        st.download_button(
            label="ğŸ“ˆ å…¨ã¦ã®å±¥æ­´ã‚’PDFã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=pdf_data,
            file_name=f"gaze_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
            mime="application/pdf",
        )

    for item in reversed(st.session_state.history):
        with st.expander(f"{item['time']} ã®çµæœ"):
            s = item["stats"]
            rows = [("â‘  å·¦ä¸Š", s[1]), ("â‘¡ å³ä¸Š", s[2]), ("â‘¢ å·¦ä¸‹", s[3]), ("â‘£ å³ä¸‹", s[4])]
            st.table({"ç¯„å›²": [r[0] for r in rows], "åˆè¨ˆç§’": [f"{r[1]:.1f}" for r in rows]})
